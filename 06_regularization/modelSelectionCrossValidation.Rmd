---
title: "Model Selection by Cross Validation"
output: html_document
---

```{r}
library(ISLR2)
library(leaps)
```

Let's use the same regsubset function but change method to be "forward"

```{r}
Hitters=na.omit(Hitters)
regfit.fwd=regsubsets(Salary~., data=Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
```

Let's do Cross Validation for doing model selection. 

- We will be using 10-fold

```{r}
# Let's check the dimensions
dim(Hitters)

set.seed(11)
# Sample from 1 to 10 
# Each observation will be assigned a fold number
# Create a vector with equal numbers of 1's upto 10's
# And Shuffle
folds=sample(rep(1:10, length=nrow(Hitters)))
folds
```

Tabulating it

```{r}
table(folds)
```

- It seems pretty balanced.
- Now let's create the matrix 

```{r}
# let's use a function for prediction
predict.regsubsets=function(object, newdata, id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form, newdata)
  coefi=coef(object, id=id)
  mat[,names(coefi)] %*% coefi
}
```


```{r}
# 10 rows (folds) and 19 variables
cv.errors=matrix(NA, 10,19)

for (k in 1:10){
  # Fit subset models with training data where train will be all except fold k
  best.fit=regsubsets(Salary~., data=Hitters[folds!=k,], nvmax = 19, method="forward")
  
  for (i in 1:19){
    # Use the predict method using the fold K ( left out ones )
    pred=predict(best.fit, Hitters[folds==k,],id=i)
    # Compute MSE 
    cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2)
  }
}
```

```{r}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19, type='b')
```

- It seems we favour models between 9 to 12 features.

