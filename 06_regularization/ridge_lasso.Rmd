---
title: "Ridge Regression an Lasso"
output: html_document
---

Let's use glmnet.

```{r}
library(glmnet)
library(leaps)
library(ISLR2)
```

```{r}
Hitters=na.omit(Hitters)
```


Let's look at Ridge and Lasso over this sections. 

Notes: 
- glmnet doesn't have formula so we need to provide a matrix X of predictors and response vector Y

```{r}
x=model.matrix(Salary~.,-1, data=Hitters)
y=Hitters$Salary
```

## Ridge regression

Let's fit a Ridge Regression model. 
- glmnet has alpha=0 ==> It's a Ridge
- glmnet has alpha=1 ==> It's a Lasso


```{r}
fit.ridge=glmnet(x,y,alpha = 0)
plot(fit.ridge, xvar="lambda", label=TRUE)
```

- Makes a plot, It's plotting all coefficients. 
- Penalties are put on the sum of squares of the coefficients
- Criterion for Ridge Regresion is $RSS + \lambda * \sum_{j=i}^{p} B_j^2$
- If $\lambda$ is big, you might want sum of squares of coeff to be small so that will shrink coeff to zero.
- glmnet models with a grid of $\lambda$
- unlike subset and forwards which control complexity of model by restricting # of variables, glmnet - ridge will keep all of them
- glmnet has a built in function called cv.glmnet that does cross-val kfold 


```{r}
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

- Gives a plot of cross val MSE. 
- It seems that full model is doing a good job. 
- 2 Vertical lines:
  - 1st at the mininum
  - 2nd at one standard-error of minimum
  
## Lasso Regression 

- Similar to Ridge with the difference on the penalty as follows:

$RSS + \lambda * \sum_{j=i}^{p} |B_j^2|$

- We penalize the absolute value of coefficients. Restricting some coefficients to be exactly zero.

```{r}
fit.lasso=glmnet(x,y, alpha=1)
plot(fit.lasso, xvar="lambda", label=TRUE)
```

- You can see that initially coeff are exactly zero
- On the top, how many non-zero variables are in the model


```{r}
cv.lasso=cv.glmnet(x,y, alpha=1)
plot(cv.lasso)
```
- Tell us that the minimum cross-val error is for size 15
- With one standard-error the model is around 6

```{r}
# Pick coeff from best model
coef(cv.lasso)
```

## Using Training & Validation 


```{r}
# let's use a function for prediction
predict.regsubsets=function(object, newdata, id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form, newdata)
  coefi=coef(object, id=id)
  mat[,names(coefi)] %*% coefi
}
```

```{r}
set.seed(1)
# seq --> generates the sequence from 1 to n
# Take a sample of 180
train=sample(seq(263), 180, replace=FALSE)
train
lasso.tr=glmnet(x[train,],y[train])
lasso.tr
```

- It tries to fit 100 values of $\lambda$ but after certain point it doesn't make any progress and it stops


```{r}
pred=predict(lasso.tr,x[-train,])
dim(pred)
```

```{r}
# 1 Compute the rsme, if dims doesn't match it does recycling
rmse = sqrt(apply((y[-train]-pred)^2,2,mean))
# 2 Plot as a function of lambda
plot(log(lasso.tr$lambda), rmse, type='b', xlab="log(lambda)")
```

```{r}
# 3
lam.best=lasso.tr$lambda[order(rmse)[1]]
# 4
lam.best
```

```{r}
# 5
coef(lasso.tr,s=lam.best)
```


- Step 2: we can observe how to the bottom-left is overfitting, to the top-right is underfitting. 
- Step 3: Extract best lambda by order of rmse
- Step 5: Get the coefficients.



